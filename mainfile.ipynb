{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11555075,"sourceType":"datasetVersion","datasetId":7245480},{"sourceId":166218,"sourceType":"modelInstanceVersion","modelInstanceId":141432,"modelId":164048},{"sourceId":166245,"sourceType":"modelInstanceVersion","modelInstanceId":141458,"modelId":164048}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Installing dependencies\n!pip install langchain\n!pip install langchain-community\n!pip install torch\n!pip install transformers\n!pip install huggingface-hub\n!pip install sentence_transformers\n!pip install faiss-cpu\n!pip install pypdf\n!pip install kagglehub\n!pip install -U langchain-huggingface bitsandbytes accelerate\n!pip install autoawq\n#Using Streamlit Interface : Created a simple web interface\n!pip install streamlit \n!npm install localtunnel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Langchain with Qwen-0.5B model on Kaggle\n# importing dependencies\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport kagglehub\nimport torch\ntorch.cuda.empty_cache()\n# Load PDF files\nloader = PyPDFDirectoryLoader('/kaggle/input/resumepdf')\ndata = loader.load()\n\n# Dividing into chunks\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nsplits = splitter.split_documents(documents=data)\n\n# Creating embeddings\nembeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n\n# Create vector stores of chunks by converting it to embeddings\nvector_stores = FAISS.from_documents(splits, embedding=embeddings)\n\n# Download and load the different Qwen model and tokenizer\n#model_name = kagglehub.model_download(\"qwen-lm/qwq-32b/transformers/qwq-32b\")\nmodel_name = \"/kaggle/input/qwen2.5/transformers/0.5b/1\"\n#model_name = \"/kaggle/input/qwen2.5/transformers/3b/1\"\n#model_name = \"/kaggle/input/qwen2.5/transformers/3b-instruct/1\"\n#model_name = \"/kaggle/input/qwen2.5/transformers/1.5b-instruct/1\"\n#model_name = kagglehub.model_download(\"qwen-lm/qwq-32b/transformers/qwq-32b-awq\")\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Create a text generation pipeline\ngenerator = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    #depending on maxlength the response may differ- more the token more the context\n    max_length=2000,\n    num_return_sequences=1,\n    do_sample=True,\n    temperature=0.01,\n    top_p=1\n)\n\n# Wrap the pipeline with HuggingFacePipeline\nllm = HuggingFacePipeline(pipeline=generator)\n\n#Retrieve relevant documents\nretriever = vector_stores.as_retriever(search_kwargs={\"k\": 2})\nquery = \":summarize in one paragraph\"\ndocs = retriever.get_relevant_documents(query)\n\n#Combine context into dense format\ncontext = \" \".join(doc.page_content.replace(\"\\n\", \" \").strip() for doc in docs)\n\n#Prepare custom prompt\ncustom_prompt = f\"\"\"\nYou are a helpful assistant. Given the following context, please answer the question concisely.\nContext:{context}\nQuestion: {query}\nAnswer: \"\"\"\n\n# Generate the answer\nresponse = generator(custom_prompt, return_full_text=False)[0][\"generated_text\"]\n\nprint(f\"context: {context}\")\nprint(\"--------------------------------\")\nprint(f\"question: {query}\")\nprint(f\"answer: {response.strip()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:10:03.746290Z","iopub.execute_input":"2025-04-25T06:10:03.746655Z","iopub.status.idle":"2025-04-25T06:10:12.310768Z","shell.execute_reply.started":"2025-04-25T06:10:03.746632Z","shell.execute_reply":"2025-04-25T06:10:12.309875Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nBoth `max_new_tokens` (=2048) and `max_length`(=2000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"context: cycles.  Software  Intern                                                                                                                 Biratnagar,  Nepal   AIT  Technology                                                                                                                          Dec  2020  –  Feb  2021   ●  Completed  a  Bachelor's  thesis  as  part  of  academic  research.  ●  Received  training  in  Python,  JavaScript,  and  web  technologies.  ICT  Trainer         Biratnagar,  Nepal  Biratnagar  Metropolitan  Dec  2018  –  May  2019   ●  Organize  and  clean  collected  data  for  distribution  of  Social  Security  Fund.   ●  Provided  a  3-month  ICT  training  program  for  metropolitan  and  ward  employees.   EDUCATION Santosh  Premi  Adhikari   Wurzburg,  Germany  |  Mob.  015754394063   |   Email |  LinkedIn |  Website  |  Github  An  enthusiastic  software  developer  with  a  background  in  React.js,  Node.js,  and  Python,  who  is  constantly  driven   to   acquire   new   knowledge   and   keep   up-to-date   with   the   latest   advancements   in   technology.    SKILLS   Front  End:  Javascript,  Reactjs  Back  End:  Node.js,  Python  Database:  PostgreSQL     PROJECT\n--------------------------------\nquestion: :summarize in one paragraph\nanswer: The person mentioned in the context is a software developer with a background in React.js, Node.js, and Python. They are also an experienced developer with a passion for technology and constantly seeking new knowledge and advancements in the field. They have a strong background in front-end development using Javascript, React.js, and Node.js, as well as a deep understanding of PostgreSQL database. They have completed a Bachelor's thesis as part of academic research and have organized and cleaned collected data for distribution of Social Security Fund. They have also provided a 3-month ICT training program for metropolitan and ward employees.\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"%%writefile app.py\n#Using Streamlit Interface : Created a simple web interface\n# Importing dependencies\nimport os\nimport logging\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport kagglehub\nimport streamlit as st\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\n# Load PDF files\ndef load_pdfs(directory):\n    try:\n        loader = PyPDFDirectoryLoader(directory)\n        data = loader.load()\n        logging.info(f\"Loaded {len(data)} documents.\")\n        return data\n    except Exception as e:\n        logging.error(f\"Failed to load PDFs: {e}\")\n        return []\n\n# Dividing into chunks\ndef split_documents(documents):\n    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n    splits = splitter.split_documents(documents=documents)\n    logging.info(f\"Split into {len(splits)} chunks.\")\n    return splits\n\n# Creating embeddings\ndef create_embeddings(splits):\n    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n    vector_stores = FAISS.from_documents(splits, embedding=embeddings)\n    logging.info(\"Created vector stores.\")\n    return vector_stores\n\n# Download and load the Qwen-2.5B model and tokenizer\ndef load_model():\n    try:\n        model_name = \"/kaggle/input/qwen2.5/transformers/0.5b/1\"\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=\"auto\",\n            device_map=\"auto\"\n        )\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        logging.info(\"Model and tokenizer loaded successfully.\")\n        return model, tokenizer\n    except Exception as e:\n        logging.error(f\"Failed to load model and tokenizer: {e}\")\n        return None, None\n\n# Create a text generation pipeline\ndef create_pipeline(model, tokenizer):\n    try:\n        generator = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            max_length=268,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.01,\n            top_p=1\n        )\n        logging.info(\"Text generation pipeline created.\")\n        return generator\n    except Exception as e:\n        logging.error(f\"Failed to create pipeline: {e}\")\n        return None\n\n# Wrap the pipeline with HuggingFacePipeline\ndef wrap_pipeline(generator):\n    llm = HuggingFacePipeline(pipeline=generator)\n    logging.info(\"Pipeline wrapped with HuggingFacePipeline.\")\n    return llm\n\n# Create the QA chain\ndef create_qa_chain(llm, vector_stores):\n    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_stores.as_retriever(search_kwargs={\"k\": 2}))\n    logging.info(\"QA chain created.\")\n    return qa\n\n# Run the query\ndef run_query(qa, query):\n    try:\n        result = qa.run(query)\n        logging.info(f\"Query processed: {query}\")\n        return result\n    except Exception as e:\n        logging.error(f\"Failed to process query: {e}\")\n        return str(e)\n\n# Streamlit app\ndef main():\n    st.title(\"PDF Q/A with Qwen-2.5B\")\n    \n    # File upload\n    uploaded_files = st.file_uploader(\"Upload PDF files\", type=[\"pdf\"], accept_multiple_files=True)\n    \n    if uploaded_files:\n        # Save uploaded files to a temporary directory\n        temp_dir = \"/tmp/pdfs\"\n        os.makedirs(temp_dir, exist_ok=True)\n        for file in uploaded_files:\n            file_path = os.path.join(temp_dir, file.name)\n            with open(file_path, \"wb\") as f:\n                f.write(file.getbuffer())\n        \n        # Load PDFs\n        documents = load_pdfs(temp_dir)\n        if not documents:\n            st.error(\"Failed to load PDFs.\")\n            return\n        \n        # Split documents\n        splits = split_documents(documents)\n        if not splits:\n            st.error(\"Failed to split documents.\")\n            return\n        \n        # Create embeddings\n        vector_stores = create_embeddings(splits)\n        if not vector_stores:\n            st.error(\"Failed to create embeddings.\")\n            return\n        \n        # Load model and tokenizer\n        model, tokenizer = load_model()\n        if not model or not tokenizer:\n            st.error(\"Failed to load model and tokenizer.\")\n            return\n        \n        # Create pipeline\n        generator = create_pipeline(model, tokenizer)\n        if not generator:\n            st.error(\"Failed to create pipeline.\")\n            return\n        \n        # Wrap pipeline\n        llm = wrap_pipeline(generator)\n        if not llm:\n            st.error(\"Failed to wrap pipeline.\")\n            return\n        \n        # Create QA chain\n        qa = create_qa_chain(llm, vector_stores)\n        if not qa:\n            st.error(\"Failed to create QA chain.\")\n            return\n        \n        # Query input\n        query = st.text_input(\"Enter your query:\")\n        if st.button(\"Submit\"):\n            if query:\n                result = run_query(qa, query)\n                st.success(\"Answer:\")\n                st.markdown(result)\n            else:\n                st.warning(\"Please enter a query.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:38:08.120031Z","iopub.execute_input":"2025-04-25T06:38:08.120406Z","iopub.status.idle":"2025-04-25T06:38:08.130545Z","shell.execute_reply.started":"2025-04-25T06:38:08.120379Z","shell.execute_reply":"2025-04-25T06:38:08.129823Z"}},"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"print('Use this as a tunnel password:') \n!curl ipv4.icanhazip.com\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:38:10.624738Z","iopub.execute_input":"2025-04-25T06:38:10.625464Z","iopub.status.idle":"2025-04-25T06:38:10.869598Z","shell.execute_reply.started":"2025-04-25T06:38:10.625434Z","shell.execute_reply":"2025-04-25T06:38:10.868820Z"}},"outputs":[{"name":"stdout","text":"Use this as a tunnel password:\n34.41.103.80\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501\n\n# --browser.gatherUsageStats false --server.port 8501 --server.address 0.0.0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T06:38:16.978380Z","iopub.execute_input":"2025-04-25T06:38:16.978685Z"}},"outputs":[{"name":"stdout","text":"\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://long-tools-shake.loca.lt\n","output_type":"stream"}],"execution_count":null}]}